---
layout: post
title: äººå·¥æ™ºèƒ½-æ‰‹å†™è¯†åˆ«å­¦ä¹ 
---

## pytorch æ™®é€šå®ç°

---

- ä¾èµ–åº“å®‰è£…

```python
pip install numpy torch torchvision matplotlib
```

### å®Œæ•´ä»£ç 

---

```python
import torch
from torch.utils.data import DataLoader
from torchvision import transforms
from torchvision.datasets import MNIST
import matplotlib.pyplot as plt


class Net(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.fc1 = torch.nn.Linear(28*28, 64)
        self.fc2 = torch.nn.Linear(64, 64)
        self.fc3 = torch.nn.Linear(64, 64)
        self.fc4 = torch.nn.Linear(64, 10)

    def forward(self, x):
        x = torch.nn.functional.relu(self.fc1(x))
        x = torch.nn.functional.relu(self.fc2(x))
        x = torch.nn.functional.relu(self.fc3(x))
        x = torch.nn.functional.log_softmax(self.fc4(x), dim=1)
        return x


def get_data_loader(is_train):
    to_tensor = transforms.Compose([transforms.ToTensor()])
    data_set = MNIST("", is_train, transform=to_tensor, download=True)
    # ä¸€ä¸ªæ‰¹æ¬¡åŒ…æ‹¬15å¼ å›¾ç‰‡
    return DataLoader(data_set, batch_size=15, shuffle=True)


def evaluate(test_data, net):
    n_correct = 0
    n_total = 0
    with torch.no_grad():
        for (x, y) in test_data:
            outputs = net.forward(x.view(-1, 28*28))
            for i, output in enumerate(outputs):
                if torch.argmax(output) == y[i]:
                    n_correct += 1
                n_total += 1
    return n_correct / n_total


def main():

    train_data = get_data_loader(is_train=True)
    test_data = get_data_loader(is_train=False)
    net = Net() # åˆå§‹åŒ–ç¥ç»ç½‘ç»œ

    print("initial accuracy:", evaluate(test_data, net))
    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)
    # epochæ˜¯è®­ç»ƒè¯¥æ•°æ®é›†çš„è½®æ¬¡ï¼Œæé«˜æ•°æ®é›†çš„åˆ©ç”¨ç‡
    for epoch in range(2):
        for (x, y) in train_data:
            net.zero_grad()
            output = net.forward(x.view(-1, 28*28))
            loss = torch.nn.functional.nll_loss(output, y)
            loss.backward()
            optimizer.step()
        print("epoch", epoch, "accuracy:", evaluate(test_data, net))

    for (n, (x, _)) in enumerate(test_data):
        if n > 3:
            break
        predict = torch.argmax(net.forward(x[0].view(-1, 28*28)))
        plt.figure(n)
        plt.imshow(x[0].view(28, 28))
        plt.title("prediction: " + str(int(predict)))
    plt.show()


if __name__ == "__main__":
    main()
```

## å·ç§¯ç¥ç»ç½‘ç»œå®ç°

---

- å¢™è£‚æ¨èï¼š[ä»â€œå·ç§¯â€ã€åˆ°â€œå›¾åƒå·ç§¯æ“ä½œâ€ã€å†åˆ°â€œå·ç§¯ç¥ç»ç½‘ç»œâ€ï¼Œâ€œå·ç§¯â€æ„ä¹‰çš„ 3 æ¬¡æ”¹å˜](https://www.bilibili.com/video/BV1VV411478E/?spm_id_from=333.337.search-card.all.click&vd_source=519c4464a364b8611b8a226be3cda0f6)

  ä¸ä»…æ•™ç»™æˆ‘ä»¬å·ç§¯çš„çŸ¥è¯†ï¼Œè¿˜æ•™ç»™æˆ‘ä»¬ä¸€ç§æ€è€ƒæ–¹å¼ ğŸ¤”ï¼Œä»ç†è®ºçš„å±‚é¢åˆ†æäº†å·ç§¯ã€‚

```python
import torch # ç”¨äºå¼ é‡è®¡ç®—å’Œè‡ªåŠ¨æ±‚å¯¼
import torch.nn as nn # æä¾›ç¥ç»ç½‘ç»œæ¨¡å—ï¼ˆå·ç§¯ã€å…¨è¿æ¥å±‚ç­‰ç­‰ï¼‰
import torch.optim as optim # åŒ…å«ä¼˜åŒ–å™¨æ¨¡å—ï¼Œç”¨äºæ¢¯åº¦ä¸‹é™çš„æƒé‡æ›´æ–°
import torchvision # æä¾›è®¡ç®—æœºè§†è§‰ç›¸å…³çš„æ•°æ®é›†ã€æ¨¡å‹å’Œå›¾åƒå¤„ç†
from torchvision import datasets, transforms # datasetsæ•°æ®é›†ã€transformså¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†
from torch.utils.data import DataLoader # ç”¨äºæŒ‰æ‰¹æ¬¡åŠ è½½æ•°æ®
from torch.utils.data import Subset # ä»æ•°æ®é›†ä¸­æå–å­é›†

# æ£€æŸ¥è®¾å¤‡æ˜¯å¦å¯ç”¨cudaï¼Œå¦åˆ™ä½¿ç”¨CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# è¶…å‚æ•°
batch_size = 100 # æ¯æ¬¡è¿­ä»£å¤„ç†çš„æ ·æœ¬æ•°é‡ï¼Œä¸€æ‰¹
learning_rate = 0.001 # æ§åˆ¶æ¯æ¬¡æƒé‡æ›´æ–°çš„æ­¥é•¿
epochs = 2 # è®­ç»ƒå®Œæ•´æ•°æ®é›†è½®æ•°

# æ•°æ®é¢„å¤„ç†
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  # å°†æ•°æ®å½’ä¸€åŒ–åˆ° [-1, 1]
])

# ä¸‹è½½ MNIST æ•°æ®é›†
train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)

# æå–æ•°æ®é›†å­é›†
# åˆ›å»ºä¸€ä¸ªåŒ…å«å‰ 55,000 æ¡æ•°æ®çš„å­é›†
subset_indices = list(range(55000))  # å‰ 55,000 æ¡æ•°æ®çš„ç´¢å¼•
train_subset = Subset(train_dataset, subset_indices)

# æ•°æ®åŠ è½½å™¨
train_loader = DataLoader(dataset=train_subset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)

# å®šä¹‰ CNN æ¨¡å‹
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.fc1 = nn.Linear(64 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, 10)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.conv1(x))  # ç¬¬ä¸€å±‚å·ç§¯ + æ¿€æ´»
        x = self.pool(x)             # ç¬¬ä¸€å±‚æ± åŒ–
        x = self.relu(self.conv2(x)) # ç¬¬äºŒå±‚å·ç§¯ + æ¿€æ´»
        x = self.pool(x)             # ç¬¬äºŒå±‚æ± åŒ–
        x = x.view(x.size(0), -1)    # å±•å¹³
        x = self.relu(self.fc1(x))   # å…¨è¿æ¥å±‚ + æ¿€æ´»
        x = self.fc2(x)              # è¾“å‡ºå±‚
        return x

# å®ä¾‹åŒ–æ¨¡å‹
model = CNN().to(device)

# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# è®­ç»ƒæ¨¡å‹
def train(model, train_loader, criterion, optimizer, device):
    model.train() # åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼
    total_batches = len(train_loader)  # è®¡ç®—æ¯ä¸ª epoch çš„æ€»æ‰¹æ¬¡æ•°
    for epoch in range(epochs):
        total_loss = 0
        for batch_idx, (images, labels) in enumerate(train_loader):
            images, labels = images.to(device), labels.to(device)

            # å‰å‘ä¼ æ’­
            outputs = model(images)
            loss = criterion(outputs, labels)

            # åå‘ä¼ æ’­å’Œä¼˜åŒ–
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
             # æ¯ 50 æ‰¹æ¬¡è¾“å‡ºä¸€æ¬¡è®­ç»ƒè¯¯å·®
            if (batch_idx + 1) % 50 == 0:
                print(f"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx+1}/{total_batches}], Loss: {loss.item():.4f}")


        print(f"Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(train_loader):.4f}")

# æµ‹è¯•æ¨¡å‹
def test(model, test_loader, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    print(f"Test Accuracy: {100 * correct / total:.2f}%")

# è¿è¡Œè®­ç»ƒå’Œæµ‹è¯•
train(model, train_loader, criterion, optimizer, device)
test(model, test_loader, device)
```
